{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhzcJeUgA78K"
   },
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsoP-N2kA78L"
   },
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__:  Nathan de Bruijn, David Wessels, Tim Frohlich, Thomas Bellucci\n",
    "\n",
    "__Student id(s)__ : 11030399, 11323272, 11233982, 11257245\n",
    "\n",
    "__Email(s)__ : nathanldebruijn@gmail.com, davidwessels15@gmail.com ,  timfrohlich@hotmail.com, th.bellucci@gmail.com\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src=https://i.imgur.com/5agchtw.jpgm height=\"300\" width=\"300\">\n",
    "\n",
    "<img src=https://i.imgur.com/kBjXR2Q.jpg height=\"300\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YBnZstHA78M"
   },
   "source": [
    "        \n",
    "<h3>Abstract</h3>\n",
    "<p>We will do text classification on a collection of Dutch parliamentary questions.\n",
    "    The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a> lets you search in \"kamervragen\".\n",
    "    <!--You can donwload\n",
    "    <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a>\n",
    "    to see some of the  data in XML format. \n",
    "    It also contains style sheets to show the XML well in a browser.  \n",
    "-->\n",
    "    The <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/'>MYSQL directory</a> contains an <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR14807.xml'>example   Kamervraag XML file</a> and a file `kvr.csv.gz` with 40K kamervragen in a handy csv format.  \n",
    "  \n",
    " We will use this Kamer Vragen CSV to build a multinomial Naive Bayes classifier from scratch to predict which ministry a novel \"kamervraag\" belongs to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmMD925yLkX9"
   },
   "source": [
    "### Imports :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LSJdgOrnKTbO",
    "outputId": "7209ef15-d26e-455a-856d-dabe93fe4453"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# install unidecode on colab \n",
    "!pip install -q unidecode\n",
    "\n",
    "# data structures\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# for sklearn implementation of question 7\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, chi2\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# IO\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# linguistic processing\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# math\n",
    "import numpy as np\n",
    "\n",
    "# makes sure that all resources that are required are downloaded\n",
    "nltk.download('punkt')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8H5rb73A78M"
   },
   "source": [
    "### Setup: Loading the Kamer Vragen CSV into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "Qd4oXS7_A78N",
    "outputId": "3a8b2502-9093-4a3d-9d5d-f548594fbcde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mShape of the KVR DataFrame\u001b[0m: 40516 rows by 6 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaar</th>\n",
       "      <th>partij</th>\n",
       "      <th>titel</th>\n",
       "      <th>vraag</th>\n",
       "      <th>antwoord</th>\n",
       "      <th>ministerie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V000113490.xml</th>\n",
       "      <td>2001</td>\n",
       "      <td>VVD</td>\n",
       "      <td>Vragen over verschil in behandeling tussen ou...</td>\n",
       "      <td>Bent u bekend met het feit dat nieuw toegelat...</td>\n",
       "      <td></td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR21909.xml</th>\n",
       "      <td>2004</td>\n",
       "      <td>SP</td>\n",
       "      <td>Vragen naar aanleiding van een bericht in de ...</td>\n",
       "      <td>Bent u bekend met het bericht over de dood va...</td>\n",
       "      <td>Ja. Bij de regiopolitie Friesland is door nab...</td>\n",
       "      <td>Volksgezondheid, Welzijn en Sport (VWS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0000107400.xml</th>\n",
       "      <td>1987</td>\n",
       "      <td>CDA</td>\n",
       "      <td></td>\n",
       "      <td>Is het de staatssecretaris bekend, dat een Ne...</td>\n",
       "      <td>Het is mij bekend dat diverse deelstaten van ...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR12266.xml</th>\n",
       "      <td>2000</td>\n",
       "      <td>SGP</td>\n",
       "      <td>Vragen naar aanleiding van het bericht in de ...</td>\n",
       "      <td>Kent u het bericht dat de Europese Commissie ...</td>\n",
       "      <td>Ja. Dit is niet geheel correct. De Commissie ...</td>\n",
       "      <td>Buitenlandse Zaken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KVR19333.xml</th>\n",
       "      <td>2003</td>\n",
       "      <td>GroenLinks</td>\n",
       "      <td>Mensen met deels een uitkering en deels een i...</td>\n",
       "      <td>Herinnert u zich uw antwoorden op de vragen 8...</td>\n",
       "      <td></td>\n",
       "      <td>Sociale Zaken en Werkgelegenheid (SZW)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  jaar       partij  \\\n",
       " V000113490.xml   2001          VVD   \n",
       " KVR21909.xml     2004           SP   \n",
       " 0000107400.xml   1987          CDA   \n",
       " KVR12266.xml     2000          SGP   \n",
       " KVR19333.xml     2003   GroenLinks   \n",
       "\n",
       "                                                             titel  \\\n",
       " V000113490.xml   Vragen over verschil in behandeling tussen ou...   \n",
       " KVR21909.xml     Vragen naar aanleiding van een bericht in de ...   \n",
       " 0000107400.xml                                                      \n",
       " KVR12266.xml     Vragen naar aanleiding van het bericht in de ...   \n",
       " KVR19333.xml     Mensen met deels een uitkering en deels een i...   \n",
       "\n",
       "                                                             vraag  \\\n",
       " V000113490.xml   Bent u bekend met het feit dat nieuw toegelat...   \n",
       " KVR21909.xml     Bent u bekend met het bericht over de dood va...   \n",
       " 0000107400.xml   Is het de staatssecretaris bekend, dat een Ne...   \n",
       " KVR12266.xml     Kent u het bericht dat de Europese Commissie ...   \n",
       " KVR19333.xml     Herinnert u zich uw antwoorden op de vragen 8...   \n",
       "\n",
       "                                                          antwoord  \\\n",
       " V000113490.xml                                                      \n",
       " KVR21909.xml     Ja. Bij de regiopolitie Friesland is door nab...   \n",
       " 0000107400.xml   Het is mij bekend dat diverse deelstaten van ...   \n",
       " KVR12266.xml     Ja. Dit is niet geheel correct. De Commissie ...   \n",
       " KVR19333.xml                                                        \n",
       "\n",
       "                                               ministerie  \n",
       " V000113490.xml                                       nan  \n",
       " KVR21909.xml     Volksgezondheid, Welzijn en Sport (VWS)  \n",
       " 0000107400.xml                                       nan  \n",
       " KVR12266.xml                          Buitenlandse Zaken  \n",
       " KVR19333.xml      Sociale Zaken en Werkgelegenheid (SZW)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init(n=False):\n",
    "    names = ['jaar', 'partij','titel','vraag','antwoord','ministerie']\n",
    "    fname = 'http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR.csv.gz'\n",
    "  \n",
    "    # optionally: load a subset of the data if loading it in full takes too long\n",
    "    kvrdf = None\n",
    "    if n:\n",
    "        kvrdf = pd.read_csv(fname, compression='gzip', sep='\\t', index_col=0, names=names, nrows=n) \n",
    "    else:\n",
    "        kvrdf = pd.read_csv(fname, compression='gzip', sep='\\t', index_col=0, names=names) \n",
    "    \n",
    "    # shuffle data to break up odd temporal inconsistencies in the data \n",
    "    kvrdf = kvrdf.sample(frac=1)\n",
    "    \n",
    "    # replace NaNs with the string 'nan'\n",
    "    kvrdf.fillna('nan')\n",
    "\n",
    "    for kolom in names[1:]:\n",
    "        kvrdf[kolom]= kvrdf[kolom].astype(str)\n",
    "    return kvrdf\n",
    "\n",
    "\n",
    "kvrdf = init()\n",
    "print('\\033[1mShape of the KVR DataFrame\\033[0m: {} rows by {} columns'.format(kvrdf.shape[0], kvrdf.shape[1]))\n",
    "kvrdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TiJr7ZyRQe8K"
   },
   "source": [
    "---\n",
    "# Question 1:\n",
    "### Normalize the values for \"ministerie\" and choose 10 ministeries to work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "JT2bLI8KOB6M",
    "outputId": "500ec8fd-49cc-4ca5-8189-3a06ffab5f47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/22251 [00:00<01:53, 195.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique ministries before normalization: 428\n",
      "Number of unique ministries after normalization:   121\n",
      "Number of \"kamer vragen\" remaining after removal: 22251 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 11878/22251 [01:07<00:55, 185.69it/s]"
     ]
    }
   ],
   "source": [
    "# creates a list of normalized terms from a document\n",
    "def clean_combined(doc):\n",
    "    return [unidecode(w) for w in word_tokenize(doc.lower()) if w.isalpha()] \n",
    "\n",
    "# returns a list of unique tokens in a document\n",
    "def get_set(combined):\n",
    "    return list(set(combined))\n",
    "  \n",
    "# converts the DataFrame to dictionary format\n",
    "def DataFrame_to_dict(df, ratio=0.8):\n",
    "    # maintain dictionaries for training and testing data sets\n",
    "    train_data, test_data = defaultdict(dict), defaultdict(dict)\n",
    "    \n",
    "    # loop through the dataframe\n",
    "    for i, item in enumerate(df.iterrows()):\n",
    "        index, row = item        \n",
    "        c = row['ministerie']\n",
    "\n",
    "        # tokenize and store in training or testing dict\n",
    "        if i < df.shape[0]*0.8:\n",
    "            train_data[c][index.strip()] = row['combined']\n",
    "        else:\n",
    "            test_data[c][index.strip()] = row['combined']\n",
    "    return train_data, test_data\n",
    "  \n",
    "  \n",
    "  \n",
    "# merge columns titel, vraag and antwoord to define a document\n",
    "df = kvrdf\n",
    "df[\"combined\"] = df.titel + df.vraag + df.antwoord\n",
    "\n",
    "# remove data containing NaNs\n",
    "df = df[[\"combined\",\"ministerie\"]]\n",
    "df = df[df['ministerie'] != 'nan']\n",
    "\n",
    "# calculate number of unique ministries before preprocessing\n",
    "pre = len(df.ministerie.value_counts().index)\n",
    "\n",
    "# normalization of ministry/class names\n",
    "df.ministerie = df.ministerie.apply(unidecode).str.lower().str.replace(r'\\(.*\\)',\"\").str.strip()\n",
    "\n",
    "# pick the then most occurring ministries to work with\n",
    "ten_classes = list(df.ministerie.value_counts().head(10).index)\n",
    "\n",
    "print(\"\\nNumber of unique ministries before normalization:\",pre)\n",
    "print(\"Number of unique ministries after normalization:  \",len(df.ministerie.value_counts().index))\n",
    "\n",
    "# remove irrelevant df rows (that do not have the ministries we use)\n",
    "df = df[df['ministerie'].isin(ten_classes)]\n",
    "print('Number of \"kamer vragen\" remaining after removal:', len(df), '\\n')\n",
    "\n",
    "# normalize and tokenize document text\n",
    "df.combined = df.combined.progress_apply(clean_combined)\n",
    "df[\"word_set\"] = df.combined.progress_apply(get_set)\n",
    "df[\"combo_string\"] = df['word_set'].str.join(' ')\n",
    "\n",
    "# create training and testing dictionaries with a ratio 0f 80%-20% from df\n",
    "train_data, test_data = DataFrame_to_dict(df, ratio=0.8)\n",
    "\n",
    "# print some additional stats\n",
    "print('\\nTraining data contains exactly {} classes'.format(len(train_data)))\n",
    "print('Test data contains the same {} classes\\n'.format(len(test_data)))\n",
    "print('\\033[1mThe ten classes we will use (with training set counts)\\033[0m:')\n",
    "for c, docs in sorted(train_data.items(), key=lambda x: -len(x[1])):\n",
    "    print('{} (n={})'.format(c, len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AUd5dbnFzr2"
   },
   "source": [
    "**Defining what constitutes the document and the class:**<br>\n",
    "The CSV file has been loaded into a pandas ``DataFrame``.  Each row of the DataFrame consists of a single document. The last item of each row/document contains the \"ministerie\" which acts as our class label (used for training the Naive Bayes classifier). The document that we would like to classify consists of a concatenation of the \"vraag\", \"antwoord\" and \"titel\" entries of the document. \"Jaar\" and \"partij\" entries are discarded.\n",
    "\n",
    "**Normalizing the content in the DataFrame:**<br>\n",
    "To be able to use the documents for our Naive Bayes classifier it is necessary to normalize the raw tokens of the text and labels such that different surface forms of the same term get mapped to the same equivalence class. Also, the string of text has to be tokenized (using NLTK's ``word_tokenize``) such that we can determine the multinomial distribution for the classifier. The following normalization steps have been performed on the <i>kamervragen</i> DataFrame as loaded in the ``Setup`` section:\n",
    "\n",
    "* To solve the problem of having different cases in the data by mapping every token to lowercase (e.g. \"Justitie\" to \"justitie\"). This is done for both the \"ministeries\" column (i.e. the classes) and the text data that represents our document.  \n",
    "* We apply ``unidecode`` to map all non-ascii characters to their closest counterpart in ascii. This has the result of removing accents and diacritics (e.g. \"financiën\" to \"financien\"). This is done for both the \"ministeries\" column (i.e. the classes) and the text data that represents our document. \n",
    "* Abbreviations are removed using regular expressions (e.g. \"justitie (JUS)\" to \"justitie\"). This is done for the classes only, because abbreviations (or anything between parentheses) can be a useful feature for the classification process.\n",
    "* Finally, all tokens containing non-alphanumeric characterrs are removed. (Note that our tokenization process takes care of non-alphanumeric characters that are stuck to the end of tokens, such as \"Nederland.\". This string gets tokenized into [\"nederland\", \".\"].)\n",
    "\n",
    "Additionally, care is taken to remove NaNs from the CSV.\n",
    "\n",
    "**Converting from DataFrame to defaultdict:**<br>\n",
    "Although often very useful, the ``DataFrame`` is not in all cases ideal when we need to do some pocessing later on in the assignment. Therefore, the DataFrame is additionally converted to a nested Python ``defaultdict`` of the following structure:\n",
    "```\n",
    "{\n",
    "    class_name1 : {\n",
    "                   doc_name1 : [t1, t2, t3, t4, ...],\n",
    "                   doc_name2 : [t1, t2, t3, t4, ...],\n",
    "                   doc_name3 : [t1, t2, t3, t4, ...],\n",
    "                   ...\n",
    "                  },\n",
    "                  \n",
    "    class_name2 : {\n",
    "                   doc_name4 : [t1, t2, t3, t4, ...],\n",
    "                   doc_name5 : [t1, t2, t3, t4, ...],\n",
    "                   doc_name6 : [t1, t2, t3, t4, ...],\n",
    "                   ...\n",
    "                  },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "This allows us to efficiently create the multinomial distributions required for Naive Bayes. During dictionary construction, it is possible to split the dataset into a training and a test set. This is done now to ensure that we do nopt train on the testing set accidentally. For our final evaluation, we use a 80%-20% split of the full dataset (excluding rows with NaNs) for the training and testing sets respectively. Before splitting, the rows are shuffled/randomized in the DataFrame to ensure that odd temporal variations are accounted for (for example when a  ministry gets another name from one year onwards).\n",
    "\n",
    "The dictionary representation of the training data is used in the following question to construct the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7nOCKbOQXvM"
   },
   "source": [
    "---\n",
    "# Question 2:\n",
    "### Implement the two algorithms in Fig MRS.13.2, **using your earlier code for creating term and document frequencies**. It might be easier to use the representation and formula given in MRS section 13.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s1FopO2cI-WG"
   },
   "outputs": [],
   "source": [
    "def extract_vocabulary(data):\n",
    "    vocab = set()\n",
    "    for c, docs in data.items():\n",
    "        for text in docs.values():\n",
    "            vocab.update(text)\n",
    "    return set(vocab)\n",
    "\n",
    "\n",
    "def train_multinomial(data):\n",
    "    # maintain dict of prior and conditional probabilities\n",
    "    priors = dict()\n",
    "    conditionals = defaultdict(lambda: defaultdict(int))\n",
    "    # determine the vocabulary of terms\n",
    "    vocab = extract_vocabulary(data)\n",
    "    # calculate N (the number of documents in our training set)\n",
    "    n_total = sum([len(doc) for doc in data.values()])\n",
    "    for c, docs in data.items():\n",
    "        # calculate prior probability for class\n",
    "        priors[c] = len(docs)/n_total\n",
    "        # determine the term frequency of all terms in the documents belonging to class c\n",
    "        tf = Counter([t for tokens in docs.values() for t in tokens])\n",
    "        N = sum(tf.values())\n",
    "        # augment conditional dict with new smoothed probability values     \n",
    "        for term in vocab:\n",
    "            conditionals[term][c] = (tf[term]+1)/(N + len(vocab))\n",
    "    return vocab, priors, conditionals\n",
    "    \n",
    "\n",
    "def apply_multinomial(vocab, priors, conditionals, doc):\n",
    "    doc = [t for t in doc if t in vocab]\n",
    "    scores = []\n",
    "    # loop through classes and calculate the log probability of P(c|d)\n",
    "    for c in priors.keys():\n",
    "        conditional = 0\n",
    "        for t in doc:\n",
    "            conditional += np.log(conditionals[t][c])\n",
    "        scores.append((np.log(priors[c]) + conditional, c))\n",
    "    # return c_map\n",
    "    return max(scores)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-3m8Us_O-Qi"
   },
   "source": [
    "The implementation in the code cell above is conceptually similar to the pseudo-code in MRS figure 13.2.\n",
    "\n",
    "**Vocabulary construction:**<br>\n",
    "Before creating the multinomial distributions for our classes/ministries, we need to fix our vocabulary. To this end we loop through each document in the training set and store the occurring tokens (or terms because they are already normalized) in a Python set.\n",
    "\n",
    "**Training the Naive Bayes classifier:**<br>\n",
    "The parameter estimation / classifier training stage of ``train_multinomial`` consists of two stages:<br>\n",
    "1) Estimating the prior probabilities of the classes by estimating $P(c) = \\frac{N_c}{N}$, where $N_c$ is the length of the set of documents belonging to class $c$.<br>\n",
    "2) Estimating the conditional probabilities of the term in a class using $P(t|c) = \\frac{tf_{t, d_c}}{len(d_c)}$, where $d_c$ is the merged document consisting of all documents in class $c$. The function maintains a nested defaultdict which allows us to determine and access the conditional probability of a term with respect to the class in an efficient manner.<br>\n",
    "Additionally, we smooth the conditional probability using add-one (Laplace) smoothing to minimize the estimation problem and zero-probability problem when using terms from the vocabulary that do not occur in the class (the reason why we pre-compute the vocabulary).\n",
    "\n",
    "**Applying the classifier:**<br>\n",
    "To apply our classifier we have implemented the <i>Maximum A Posteriori method</i> $c_{map} = argmax_{c\\in C} P(c|d) = argmax_{c\\in C} P(c)\\prod_{t\\in d}P(t|c)$, where $d$ is the document we wish to classify and $c$ the class. The function maintains the calculated quantity of $P(c|d)$ for each class and returns the class name with the maximum conditional probability of P(c|d). To prevent arithmetic underflow, we apply a log transformation to our probabilities and sum them instead.<br>\n",
    "Care is taken to ensure that all terms/features that are used are in fact in the vocabulary (out-of-vocabulary tokens that may occur in the testing set do not have a probability value assigned to them, they should therefore be discarded). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaCJzqB6Qrbc"
   },
   "source": [
    "---\n",
    "# Question 3:\n",
    "### On this collection, train NB text classifiers for 10 different classes with enough and interesting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "kbgkaxCpQ7NC",
    "outputId": "fdbbc418-4a3c-40a4-d7ec-5571889a24d7"
   },
   "outputs": [],
   "source": [
    "# creates a single multinomial Naive Bayes model\n",
    "vocab, priors, condprob = train_multinomial(train_data)\n",
    "\n",
    "# printing some examples\n",
    "print('First 7 terms in vocabulary:', sorted(list(vocab))[:7])\n",
    "print('Number of terms in vocabulary:', len(vocab))\n",
    "print('\\nPriors: p(\"justitie\") =', priors['justitie'])\n",
    "print('\\ncondprob: p(\"het\"|\"justitie\") =', condprob['het']['justitie'])\n",
    "\n",
    "# testing the classifier on three example documents\n",
    "texts = ['de opleiding van studenten van de universiteit van amsterdam',\n",
    "         'werkgelegenheid jongeren .',\n",
    "         'vraag over drukte op de wegen richting amsterdam']\n",
    "\n",
    "print('\\n\\033[1mExample classifications\\033[0m:')\n",
    "for text in texts:\n",
    "    c = apply_multinomial(vocab, priors, condprob, text.split())\n",
    "    print('\"{}\" -> class = {}'.format(text, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mcKWCoD4UEMG"
   },
   "source": [
    "**Training the classifier on the training set:**<br>\n",
    "The code in the code cell above simply calls the ``train_multinomial`` function with the necessary parameters to estimate for each class $c$ a multinomial distribution over the vocabulary $V$ (so actually, a single multi-class classifier instead of multiple single-class/binary classifiers). Furthermore, it prints some examples, such as the first vocabulary terms (in a sorted manner), the class priors and some classifications using toy examples and the ``apply_multinomial`` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2OFzYjAgQtio"
   },
   "source": [
    "---\n",
    "# Question 4:\n",
    "### Compute for each term and each of your 10 classes its utility for that class using mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NccWNZeQ7-p"
   },
   "outputs": [],
   "source": [
    "def calc_mutual_info(df_train, V,classes):\n",
    "    mutual_info_dict = {key: dict.fromkeys(classes) for key in list(V)}\n",
    "\n",
    "    for c in classes:\n",
    "        sub_set = df_train[df_train['ministerie'] == c]\n",
    "        class_docs = '\\n'.join(list(sub_set.combo_string)).split()\n",
    "        class_bow = Counter(class_docs)\n",
    "        \n",
    "        complement_set = df_train[df_train['ministerie'] != c]\n",
    "        complement_docs = '\\n'.join(list(complement_set.combo_string)).split()\n",
    "        complement_bow = Counter(complement_docs)\n",
    "                \n",
    "        for t in V:\n",
    "          \n",
    "            # number of docs that contain t and  are in the class\n",
    "            n11 = class_bow[t]\n",
    "            \n",
    "            # number of docs containing t that are not in the class\n",
    "            n10 = complement_bow[t]\n",
    "            \n",
    "            # number of docs that dont contain t that are in the class\n",
    "            n01 = len(sub_set) - n11\n",
    "            \n",
    "            # number of docs that dont contain t and that are not in the class\n",
    "            n00 = len(complement_set)-n10\n",
    "            \n",
    "            N = n10+n00+n11+n01\n",
    "            \n",
    "            n1punt = n11 + n01\n",
    "            npunt1 = n11 + n10\n",
    "            n0punt = n10 + n00\n",
    "            npunt0 = n01 + n00\n",
    "                       \n",
    "            if 0 not in [npunt1,n1punt,npunt0 ,n0punt,n10,n11,n00,n01]:\n",
    "                p1 = (n11/N)*np.log2((N*n11)/(n1punt*npunt1))\n",
    "                p2 = (n10/N)*np.log2((N*n10)/(n0punt*npunt1))\n",
    "                p3 = (n01/N)*np.log2((N*n01)/(n1punt*npunt0))\n",
    "                p4 = (n00/N)*np.log2((N*n00)/(n0punt*npunt0))\n",
    "                \n",
    "                mutual_info_dict[t][c] = p1+p2+p3+p4\n",
    "\n",
    "    return mutual_info_dict \n",
    "\n",
    "# making the train,test split for the dataframe\n",
    "train_size = int(df.shape[0]*0.8)\n",
    "df_train, df_test = df.head(train_size), df.tail(len(df)-train_size)\n",
    "\n",
    "# obtaining the mutual_info_dict\n",
    "mutual_info_dict = calc_mutual_info(df_train,vocab,ten_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iakdooD9mv9E"
   },
   "source": [
    "**Feature selection with mutual information:**<br>\n",
    "To estimate the utility of terms for the given classes we have calculated their mutual information with the following formula:<br>$ I(U;C) = \\frac{N_{11}}{N}log_2 \\frac{NN_{11}}{N_{1.}N_{.1}} + \\frac{N_{01}}{N}log_2 \\frac{NN_{01}}{N_{1.}N_{.0}} + \\frac{N_{10}}{N}log_2 \\frac{NN_{10}}{N_{0.}N_{.1}} + \\frac{N_{0}}{N}log_2 \\frac{NN_{0}}{N_{0.}N_{.0}} $\n",
    "\n",
    "In our implementation we start of by looping over each of our ten classes and select all the documents that belong to that class from the dataframe. After the selection of the documents we use their combo_string column, which holds a set of all the words in the document, and a counter to count the number of times each term occurs in the class. After this we obtain the complement of the class, meaning the docs which belong to all other classes, and follow the same process to obtain their counts. The next step in the function is looping over each word in the vocabulary and calculate the mutual information using the formula above. The final step is storing the mutual information for each term and each document in a dictionary and returning that dictionary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hf50oV9XQyBY"
   },
   "source": [
    "---\n",
    "# Question 5:\n",
    "### For each class, show the top 10 words as in Figure 13.7 in MRS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2275
    },
    "colab_type": "code",
    "id": "VIHN241QQ8w2",
    "outputId": "9ec1686b-ad95-45ba-868d-3181aaee23b9"
   },
   "outputs": [],
   "source": [
    "def get_best_n_words(mutual_info_dict, V, classes, n):\n",
    "    result_dict = {}\n",
    "    for c in classes:\n",
    "        best = []\n",
    "        for t in V:\n",
    "            if mutual_info_dict[t][c]:\n",
    "                best.append([t,mutual_info_dict[t][c]])\n",
    "        if n == 'all':\n",
    "            best = np.array(sorted(best, key=lambda x:x[1],reverse=True))\n",
    "        else:\n",
    "            best = np.array(sorted(best, key=lambda x:x[1],reverse=True)[:n])\n",
    "        result_dict[c] = best\n",
    "    return result_dict\n",
    "\n",
    "# get the best 10 words and their score for each class\n",
    "best_n_dict = get_best_n_words(mutual_info_dict, vocab, ten_classes, 10)\n",
    "\n",
    "# displaying the results\n",
    "np.set_printoptions(suppress=True)\n",
    "for c in best_n_dict.keys():\n",
    "    print(\"\\n\",'\\033[1m'+c+\":\\nword\\t\\t     score\"+'\\033[0m')\n",
    "    for [word,score] in best_n_dict[c]:\n",
    "        print(\"%-20s %4.10f\" % (word, float(score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CovpFXm4N3In"
   },
   "source": [
    "In the previous cell, ``get_best_n_words`` is called to extract the top 10 terms for each class with the highest mutual information score. These classes, with their top 10  terms and mutual information scores are printed in order. Looking at the results we can see that the top ranked words are words which are easily connected and mostly very common for the particular class. Which is an indication that the algorithm works properly.\n",
    "By usinf the keyword 'all' for n it is possible to have the entire vocabulary be returned for each class (required during evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wg9nvR2Qz-N"
   },
   "source": [
    "---\n",
    "# Question 6:\n",
    "### Evaluate your classifiers using Precision, Recall and F1. ( Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes. Thus do feature selection per class, and use for each class the top n best features for that class. Also show the microaverage(s) for all 10 classes together. If you like you can also present this in a figure like MRS.13.8. Then compute the F1 measure for the same number of terms as in that figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1806
    },
    "colab_type": "code",
    "id": "lmkoVtdTQ9r8",
    "outputId": "54b8e939-fd63-4f15-ba55-9cfd8aad048c"
   },
   "outputs": [],
   "source": [
    "# calculates precision, recall and f1\n",
    "def metrics(tp, fp, fn):\n",
    "    P, R, F1 = 0, 0, 0\n",
    "    if tp or fp:\n",
    "        P = tp / (tp + fp)\n",
    "    if tp or fn:\n",
    "        R = tp / (tp + fn)\n",
    "    if P or R:\n",
    "        F1 = (2*P*R) / (P+R)\n",
    "    return P, R, F1\n",
    "        \n",
    "\n",
    "def evaluation(train_data, test_data, top_features, k, disp=False):\n",
    "    train_data, test_data = deepcopy(train_data), deepcopy(test_data)   \n",
    "    if k != 'all':\n",
    "        # merge top features in one large set\n",
    "        top_features2 = set()\n",
    "        for _, arr in top_features.items():\n",
    "            top_features2 |= set(arr[:k, 0])\n",
    "              \n",
    "        # remove features from training data that do not belong to the set of top features\n",
    "        for c, docs in train_data.items():\n",
    "            for docID, doc in docs.items():\n",
    "                train_data[c][docID] = [t for t in doc if t in top_features2]\n",
    "            \n",
    "    # recalculate model using the reduced training data with only top features\n",
    "    vocab, priors, condprob = train_multinomial(train_data)\n",
    "           \n",
    "    # gather a large list of all documents from the test set\n",
    "    all_docs = []\n",
    "    for _, docs in test_data.items():\n",
    "        all_docs += list([(docID, doc) for docID, doc in docs.items()])\n",
    "                \n",
    "    # precompute predictions for all documents in test set using the newly trained model\n",
    "    preds = [(docID, apply_multinomial(vocab, priors, condprob, terms)) for docID, terms in all_docs]\n",
    "    \n",
    "    # for each class calculate precision, recall and f-measure\n",
    "    micro_tp, micro_fp, micro_fn = 0, 0, 0\n",
    "    for c, class_docs in test_data.items():    \n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for docID, pred in preds:\n",
    "            if docID in class_docs and pred == c:\n",
    "                tp += 1\n",
    "            elif docID in class_docs:\n",
    "                fn += 1\n",
    "            elif pred == c:\n",
    "                fp += 1\n",
    "                \n",
    "        if disp:\n",
    "            P, R, F1 = metrics(tp, fp, fn)\n",
    "            print('\\n{}:\\nP={}, R={}, F1={}'.format(c, P, R, F1))\n",
    "            \n",
    "        # update tp, fn and fp for micro-averaging scores\n",
    "        micro_tp += tp\n",
    "        micro_fp += fp \n",
    "        micro_fn += fn \n",
    "            \n",
    "    # calculate micro-average f1 over classes\n",
    "    P, R, F1 = metrics(micro_tp, micro_fp, micro_fn)\n",
    "    \n",
    "    if disp:\n",
    "        print('\\nMicroaveraging F1 = {}, with |V| = {}.'.format(F1, len(vocab)))\n",
    "    else:\n",
    "        return len(vocab), F1\n",
    "\n",
    "top_features = get_best_n_words(mutual_info_dict, vocab, ten_classes, 'all')\n",
    "\n",
    "print('\\033[1mEvaluation top 10 features per class (i.e. approx 100 terms in total):\\033[0m')\n",
    "evaluation(train_data, test_data, top_features, k=10, disp=True)    \n",
    "\n",
    "print('\\n\\n\\033[1mEvaluation top 100 features per class:\\033[0m')\n",
    "evaluation(train_data, test_data, top_features, k=100, disp=True)\n",
    "\n",
    "print('\\n\\n\\033[1mEvaluation no feature extraction:\\033[0m')\n",
    "evaluation(train_data, test_data, top_features, k='all', disp=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "id": "C6E0J9-muDCm",
    "outputId": "66395aad-0060-4a2f-a75c-7534f791d61a"
   },
   "outputs": [],
   "source": [
    "# to show to entire range of possible F1 values\n",
    "F1s, vocab_norms = [], []\n",
    "for k in tqdm(np.logspace(start=1, stop=5, base=10, num=10)):\n",
    "    # evaluate model using these top k features\n",
    "    norm_vocab, score = evaluation(train_data, test_data, top_features, k=int(k), disp=False)\n",
    "    F1s.append(score)\n",
    "    vocab_norms.append(norm_vocab)   \n",
    "    \n",
    "    \n",
    "# plot figure\n",
    "plt.plot(vocab_norms, F1s, 'b-')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Vocabulary size (log scale)')\n",
    "plt.ylabel('Micro-averaged F1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXvh8PnsfMGc"
   },
   "source": [
    "**Evaluation**:<br>\n",
    "The task is to evaluate the effectiveness of our classifier with three levels of feature selection (by means of mutual information) applied; 10 features per class, 100 features per class and no feature selection. Effectiveness is measured by precision, recall and $F_1$ for each class separately and micro-averaged over classes (by summing tp, fn and fp counts over classes).<br>\n",
    "To this end we have implemented the ``evaluation`` function which takes care of the following:<br>\n",
    "* First, before calling the ``evaluation`` function, the top features are determined for each class using ``get_best_n_words()``. As a result, $k$ features are selected per class to obtain a maximum of $10k$ features for training.\n",
    "* These features are passed on to the ``evaluation`` function in which the training set data gets pre-processed. In it, all terms that are not features (i.e. not part of the $\\approx10k$ features) are removed from the local copy of the training data. Note the additional ``disp`` parameter, which determines whether the output is returned (for the plot) or printed on screen.\n",
    "* A new model is trained on the reduced training data.\n",
    "* The estimated model is used to predict classes to testing set documents to obtain a vector of classifications for the testing set.\n",
    "* This vector is compared to the ground truth and for each class precision, recall and $F_1$ is calculated.\n",
    "* Finally, the true positives, false negatives and false positives are aggregated to estimate a micro-averaged $F_1$ score for the entire classifier over all classes.\n",
    "\n",
    "This is repeated for all three levels of feature selection. \n",
    "\n",
    "There are multiple ways to evaluate the Naive Bayes model with feature selection, but by implementing this pipeline we have opted for the \"filter method\" approach that consists of:  1) Finding the set of all features (i.e. the vocabulary), 2) selecting the best subset using mutual information, 3) training the algorithm and 4) evaluating using some evaluation metric (in our case, precision, recall and $F_1$).\n",
    "\n",
    "**Observations**: <br>\n",
    "As you can see, feature selection does not work well for our purpose to increase accuracy / micro-averaged $F_1$ (see figure in the previous cell). As from our experiments, not using feature selecting increases the accuracy of our classifier. However, we can reduce the amount of features to some value such as $k=40.000$ features to reduce model complexity and reduce memory usage (However this does decrease accuracy a bit).\n",
    "\n",
    "**Additional notes on feature selection and test set**:<br>\n",
    "In the examples in the previous cells we evaluate whether feature selection increases the accuracy of our classifier. However, in a normal setting, we would not test this on the test set, but a development set instead with more unseen data. We are not actually using the classifier in the real word and we are therefore not selecting any feature selection as part of the model itself (which should be tested together, where the feature selection level was optimized using the development set). Therefore, we proceed to evaluate feature selection the model on the test set anyway, eventhough we should in practice really be doing this on a development set if we were using feature selection as part of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-B6wiF3Q1qA"
   },
   "source": [
    "---\n",
    "# Question 7:\n",
    "## You have done the complete implementation by yourself. Congratulations! You can also use `scikit-learn` routines for all of this work. Do that. So follow [this text classification tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) and implement the same steps but now with your kamervragen dataset. Also use [mutual information feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) to select the K-best features, and compare the results as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "VXKiKaiTQ-jy",
    "outputId": "f44267db-f63d-490d-e94c-0506661152e1"
   },
   "outputs": [],
   "source": [
    "# converts the dict representation to list data and target format\n",
    "def dict_to_list(dct, class_names):\n",
    "    data, targets = [], []\n",
    "    for c, docs in dct.items():\n",
    "        # if class does not yet exist, assign integer value to class\n",
    "        if c not in class_names:\n",
    "            class_names[c] = len(class_names)\n",
    "        for doc_name, doc in docs.items():\n",
    "            targets.append(class_names[c])\n",
    "            data.append(' '.join(doc))\n",
    "    return data, targets, class_names\n",
    "\n",
    "# convert dictionary format to list format preferred by sklearn\n",
    "train_data2, train_targets, class_names = dict_to_list(train_data, dict())\n",
    "test_data2, test_targets, _ = dict_to_list(test_data, class_names)\n",
    "\n",
    "# convert documents to CountVectors (Note the parameters in CountVectorizer\n",
    "# we require min_df to ensure that the mutual information can be computed fairly quickly)\n",
    "count_vect = CountVectorizer(min_df=10)\n",
    "train_counts = count_vect.fit_transform(train_data2)\n",
    "test_counts = count_vect.transform(test_data2)\n",
    "\n",
    "print('\\033[1mFeature selection microaveraged F1 using sklearn:\\033[0m')\n",
    "for k in [10, 100, 500, 1000, 10000, 'all']:\n",
    "    \n",
    "    # select k best features using SelectKBest and mutual information\n",
    "    selector = SelectKBest(mutual_info_classif, k=k)\n",
    "    train_counts2 = selector.fit_transform(train_counts, train_targets)\n",
    "    test_counts2 = selector.transform(test_counts)\n",
    "\n",
    "    # train multinomial Naive Bayes classifier\n",
    "    clf = MultinomialNB().fit(train_counts2, train_targets)\n",
    "\n",
    "    # testing on test set\n",
    "    preds = clf.predict(test_counts2)\n",
    "    print('k = {}:'.format(k), f1_score(test_targets, preds, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ew0Q_vDfW5fJ"
   },
   "source": [
    "In the code cell above we have recreated our own implementation of Multinomial Naive Bayes using build-in models from ``scikit-learn``.  It implements the following steps:\n",
    "* Convert the training and testing sets from dictionary format to the format used by sklearn. This is simply a list of strings as the data vector and a list of class IDs for the target:\n",
    "```\n",
    "*_data = [string1, string2, ...]\n",
    "*_target = [0, 1, 3, ...], where the integer IDs correspond to class names.\n",
    "```\n",
    "The ``class_names`` dictionary created during training set conversion and containing the mapping from class name to integer ID is passed on to the conversion for the testing set to ensure that both training and testing sets use the same class IDs for the same classes.\n",
    "* The converted ``train_data`` and ``test_data`` are converted to sklearn's ``CountVectors``. This model is fitted on the training set and it transforms both tests. We found it to be crucial to use the min_df parameter to ignore features with a low collection frequency. This is because we would create enormous CountVector representations in which all class-feature pairs have to be evaluated during feature selection. This is not practical. Therefore we remove infrequent features beforehand.\n",
    "* For multiple values of $k$ and 'all' (i.e. no feature selection), the best features are selected using the ``SelectKBest`` function and the ``mutual_info_classif`` metric. \n",
    "* A Naive Bayes model is fit to the CountVectors (in which the $k$ best features are selected). \n",
    "* We predict for each document in the (reduced) test set to what class it belongs using the ``predict`` method which results in a large vector of classifications.\n",
    "* This vector is compaed to the true labels using the $F_1$ score built into sklearn.\n",
    "\n",
    "This is again done for multiple levels of feature selection. Note that the values 100, 1000 and 'all' have been chosen for $k$ instead of 10, 100 and 'all'. The reason for this is because the feature selector will select exactly $k$ features, which on average is $\\frac{k}{10}$ features for each class. In the previous code that we have implemented ourselves, we used 10, 100 and 'all' for <i>each class</i>, therefore we have to scale $k$ by 10 in the sklearn implementation to obtain a nearly identical amount of features (vocabulary size). This allows us to compare the results in the next section.\n",
    "\n",
    "Also, although we have followed the tutorial fairly closely, we have not implemented it using the additional ``TfidfTransformer``. This is because our own implementation does not use tf-idf feature vectors. In our testing, using tf-idf features decreases performance across the range of feature selection levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcIok3PvQ34c"
   },
   "source": [
    "---\n",
    "# Question 8:\n",
    "## Reflect and report briefly about your choices in this process and about the obtained results. Also reflect on the differences between the scikit learn approach and the \"own implementation approach\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oeZ2726-2nt7"
   },
   "source": [
    "The process from (nearly) raw data to the final classifier and evaluation has the following stages (as highlighted by the questions). We will discuss each stage separately:\n",
    "\n",
    "**Pre-processing and text normalization:**<br>\n",
    "We found that there was quite some variety of wordforms w.r.t. some lemma (e.g.\"Financien\", \"financien\", \"financiën\" and \"Financiën\"). Therefore, we have opted for a fairly agressive form of normalization by converting the text to lowercase, replacing all unicode characters with most similar ASCII character, word segmentation by which punctuation is separated from wordforms (e.g. \"financiën.\" to \"financiën\" and \".\"), etc. We found that we could reduce the vocabulary significantly which has the added bonus that we have to estimate less parameters for our Naive Bayes model.\n",
    "\n",
    "**Classifier design:**<br>\n",
    "For the classifier we used a dictionary representation, while we were used to it from our earlier assignment of the inverted dict and it made the implementation efficient. The place where our conditional probabilities are stored could be seen as an index; a dict with for every term of the vocabulary a dict with probabilites for every class. Furthermore, the prior probabilities are also calculated and stored in a dict. These two dicts represent the model which are used for the naive bayes classifier. To classify, the given naive bayes formula is used. For a given term, the probability for each class is calculated and the term belongs to the class with the highest probabilty. From the results we saw a pretty good working algorithm. \n",
    "\n",
    "\n",
    "**Calculating Mutual Information:**<br>\n",
    "Our implementation of the mutual information, unlike the classifiers,  makes use of  dataframes. This is partly due to the fact that it different people have implemented both questions but also because of the fact that it made the calculation of the different N values: $N_{10},N_{11},  etc.$ relatively easy.  We have  choosen to make the outer for-loop iterate over the classes and the inner for-loop over the terms because retrieving the documents that belong to the class and the complement is a relatively expensive operation. We therefore wanted to minimize the amount of times this operation had to be executed.\n",
    "\n",
    "**Evaluation of Classifier and Feature Selection:**<br>\n",
    "The resulting 10 best features of our feature selection make a lot of sense as they are all closely related to the class or \"ministerie\" they are associated with. Often the list of 10 contains atleast one word which occurs in the class name, the feature \"justitie\" for example is prominent in the class \"justitie\" while it does not occur in any of the other top 10's. This is an indication that our mutual information implementation works as it should.\n",
    "\n",
    "We designed the algorithm in line with the \"filter method\" for evaluating feature selection. In it, a model is trained on the training set in which unreliable or noisy features are removed. Obviously, other methods are possible (such as training on the regular training set and filtering the terms in the to-be classified documents), but this is the most custom manner of using mutual information for feature selection. \n",
    "\n",
    "**Observations:**<br>\n",
    "As shown in question 7, in our implementation the feature selection does not perform as well as we expected. Accuracy (or equivalently micro-averaged $F_1$) increases with the vocabulary size. Micro-averaged $F_1$ decreases when the vocabulary is reduced in both our implementation and the sci-kit implementation.\n",
    "\n",
    "**Differences between sci-kit implementations and our own implementation:**<br>\n",
    "The bare sklearn implementation is very similar to our own implementation (considering that we have intentionally not implemented the tf-idf version of the classifier).  One superficial difference is the data structure that is used as input to the training routine (in this case ``fit_transform``). The estimation of mutual information is performed using a non-parametric k-nearest neighbours approach unlike our approach using the formula specified in question 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QaZe5j3bLm-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AssignmentWeek5.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
